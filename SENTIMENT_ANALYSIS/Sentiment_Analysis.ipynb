{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenvfa52ab9cf1764ba3a0abe8214a936224",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n/home/somesh/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  \"timeout or by a memory leak.\", UserWarning\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.0min\n[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.8min\n[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  9.2min finished\n/home/somesh/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\nBest parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fa0c56747a0>}\nCV Accuracy:  0.897\nTest Accuracy:  0.899\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('Datasets/movie_data.csv',encoding = 'utf-8')\n",
    "df.head(5)\n",
    "df.shape\n",
    "df.loc[0,'review'][-50:]\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=) (?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "def tokenizer(text): \n",
    "    return text.split()\n",
    "\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text): \n",
    "    return [porter.stem(word) for word in tex.split()]\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents= None, lowercase = False, preprocessor = None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1,1)], 'vect__stop_words': [stop,None], 'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, \n",
    "                {'vect__ngram_range': [(1,1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0,100.0]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state = 0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring = 'accuracy', cv= 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter set: %s'% gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: % .3f'% gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: % .3f'%clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}